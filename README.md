# Алгоритмы анализа данных
### [Урок 1](https://github.com/xrustle/GB_ML_Theory/tree/master/Lesson_1)
* Линейная регрессия. MSE
* Метод наименьших квадратов
* Градиентный спуск
### [Урок 2](https://github.com/xrustle/GB_ML_Theory/tree/master/Lesson_2)
* Масштабирование признаков. Стандартизация и нормализация
* Переобучение и методы борьбы с ним. L1 и L2-регуляризация
* Стохастический градиентный спуск
* Коэффициент детерминации R2
### [Урок 3](https://github.com/xrustle/GB_ML_Theory/tree/master/Lesson_3)
* Линейная классификация
* Функционал ошибки в линейной классификации
    * Пороговая функция потерь
    * Экспоненциальная функция потерь
    * Квадратичная функция потерь
    * Логистическая функция потерь
* Логистическая регрессия
    * Логарифмическая функция потерь (Log Loss = кросс-энтропия)
* Оценка качества классификации
    * Матрица ошибок (Confusion Matrix)
    * Доля правильных ответов (Accuracy)
    * Точность и полнота (Precision & Recall)
    * F-мера (F1-score, F_beta-score)
    * PR-кривая (PR-AUC = Area under curve)
    * ROC-кривая (ROC-AUC)
### [Урок 4](https://github.com/xrustle/GB_ML_Theory/tree/master/Lesson_4)
* Деревья решений
* Критерий информативности
* Критерии останова
* Обрезка деревьев (прунинг)
* Работа деревьев в случае пропущенных значений
* Работа деревьев с категориальными признаками
### [Урок 5](https://github.com/xrustle/GB_ML_Theory/tree/master/Lesson_5)
* Случайный лес
* Bagging (Bootstrap aggregation)
### [Урок 6](https://github.com/xrustle/GB_ML_Theory/tree/master/Lesson_6)
* Градиентный бустинг (AdaBoost)
* Стохастический градиентный бустинг
### [Урок 7](https://github.com/xrustle/GB_ML_Theory/tree/master/Lesson_7)
* k-NN
* k-means