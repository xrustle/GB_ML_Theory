{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose tutors\n",
    "https://www.kaggle.com/c/choose-tutors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача в этом соревновании - предсказать, подойдет ли репетитор для подготовки к экзамену по математике или нет.\n",
    "\n",
    "Можно использовать только следующие пакеты.\n",
    "\n",
    "Pandas используется только для чтения и записи csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv').set_index('Id')\n",
    "df_test = pd.read_csv('test.csv').set_index('Id')\n",
    "TARGET = 'choose'\n",
    "\n",
    "X = df_train.drop(TARGET, axis=1).to_numpy()\n",
    "y = df_train[TARGET].to_numpy()\n",
    "X_test = df_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание полей в датасетах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Id** — индекс репетитора,\n",
    "<br>**age** — возраст репетитора,\n",
    "<br>**years_of_experience** — опыт репетитора в годах,\n",
    "<br>**lesson_price** — текущая цена одного занятия,\n",
    "<br>**qualification** — квалификация,\n",
    "<br>**physics** — далее идут бинарные признаки того, что репетитор также преподает дополнительные предметы,\n",
    "<br>**chemistry**,\n",
    "<br>**biology**,\n",
    "<br>**english**,\n",
    "<br>**geography**,\n",
    "<br>**history**,\n",
    "<br>**mean_exam_points** — средняя оценка за экзамен у учеников,\n",
    "<br>**choose** — целевой бинарный признак. Если значение `1`, то этот репетитор нам подходит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрикой проверки является ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score(y_true, y_score):\n",
    "    y_true_sorted = np.array([x for _, x in sorted(zip(y_score, y_true))])\n",
    "    \n",
    "    n = y_true_sorted.shape[0]\n",
    "    n_pos = np.sum(y_true_sorted)\n",
    "    n_neg = n - n_pos\n",
    "    \n",
    "    tp, fp = 0, 0\n",
    "    roc = np.zeros((n + 1, 2))\n",
    "    \n",
    "    for i, class_ in enumerate(reversed(y_true_sorted)):\n",
    "        if class_ == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "            \n",
    "        roc[i + 1, 0] = fp / n_neg\n",
    "        roc[i + 1, 1] = tp / n_pos\n",
    "    \n",
    "    score = 0\n",
    "    for i in range(n):\n",
    "        score += (roc[i + 1, 0] - roc[i, 0]) * roc[i, 1] \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Класс модели логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, n_iter=1000, alpha=1e-4, threshold=.5, l1=0, l2=0, random_state=None):\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.random_state = random_state\n",
    "        self.W = None\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_logloss(y, y_pred, epsilon=1e-8):\n",
    "        y = y.astype(np.float64)  # Привожу к типу вещественному,\n",
    "        y_pred = y_pred.astype(np.float64)  # иначе ошибка в первом примере состоящем только из целых нудей и единиц\n",
    "\n",
    "        y_pred[y_pred > 1 - epsilon] = 1 - epsilon  # Ограничим значения предсказанных значений интервалом [epsilon; 1 - epsilon]\n",
    "        y_pred[y_pred < epsilon] = epsilon\n",
    "\n",
    "        err = - np.mean(y * np.log(y_pred) + (1.0 - y) * np.log(1.0 - y_pred))\n",
    "        return err\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        res = 1 / (1 + np.exp(-z))\n",
    "        return res\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        self.W = np.random.randn(X.shape[1])\n",
    "        n = X.shape[0]\n",
    "\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            z = self.W @ X.T\n",
    "            y_pred = self.sigmoid(z)\n",
    "\n",
    "            self.W -= self.alpha * (1 / n * np.dot((y_pred - y), X) + self.l1 * np.sign(self.W) + self.l2 * self.W)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = np.dot(self.W, X.T)\n",
    "        preds = self.sigmoid(z)\n",
    "        return preds\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = (self.predict_proba(X) > self.threshold).astype('int')\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Классы для построения деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, index, t, left_branch, right_branch):\n",
    "        self.index = index  # индекс признака, по которому ведется сравнение с порогом в этом узле\n",
    "        self.t = t  # значение порога\n",
    "        self.left_branch = left_branch  # поддерево <= t\n",
    "        self.right_branch = right_branch  # поддерево > t\n",
    "\n",
    "    def classify(self, sample):\n",
    "        if sample[self.index] <= self.t:\n",
    "            return self.left_branch.classify(sample)\n",
    "        else:\n",
    "            return self.right_branch.classify(sample)\n",
    "\n",
    "    def probability(self, sample):\n",
    "        if sample[self.index] <= self.t:\n",
    "            return self.left_branch.probability(sample)\n",
    "        else:\n",
    "            return self.right_branch.probability(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        regression = False\n",
    "\n",
    "        # подсчет количества объектов разных классов\n",
    "        classes = {0: 0, 1: 0}  # сформируем словарь \"класс: количество объектов\"\n",
    "        for label in self.labels:\n",
    "            if label == 0 or label == 1:\n",
    "                classes[label] += 1\n",
    "            else:\n",
    "                regression = True\n",
    "                break\n",
    "\n",
    "        if regression:\n",
    "            self.class_ = np.mean(self.labels)\n",
    "            if self.class_ < 0:\n",
    "                self.class_ = 0\n",
    "            self.proba = self.class_\n",
    "        else:\n",
    "            #  найдем класс, количество объектов которого будет максимальным в этом листе и вернем его\n",
    "            self.class_ = max(classes, key=classes.get)\n",
    "            self.proba = classes[1] / self.labels.shape[0]\n",
    "\n",
    "    def classify(self, sample=None):\n",
    "        return self.class_\n",
    "\n",
    "    def probability(self, sample=None):\n",
    "        return self.proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_leaf=1, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.tree = None\n",
    "\n",
    "    @staticmethod\n",
    "    def gini(labels):\n",
    "        #  подсчет количества объектов разных классов\n",
    "        classes = {}\n",
    "        for label in labels:\n",
    "            if label not in classes:\n",
    "                classes[label] = 0\n",
    "            classes[label] += 1\n",
    "\n",
    "        #  расчет критерия\n",
    "        impurity = 1\n",
    "        for label in classes:\n",
    "            p = classes[label] / len(labels)\n",
    "            impurity -= p ** 2\n",
    "\n",
    "        return impurity\n",
    "\n",
    "    def quality(self, left_labels, right_labels, current_gini):\n",
    "        # доля выбоки, ушедшая в левое поддерево\n",
    "        p = float(left_labels.shape[0]) / (left_labels.shape[0] + right_labels.shape[0])\n",
    "        return current_gini - p * self.gini(left_labels) - (1 - p) * self.gini(right_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def split(data, labels, index, t):\n",
    "        left = np.where(data[:, index] <= t)\n",
    "        right = np.where(data[:, index] > t)\n",
    "        return data[left], data[right], labels[left], labels[right]\n",
    "\n",
    "    def find_best_split(self, data, labels):\n",
    "        current_gini = self.gini(labels)\n",
    "\n",
    "        best_quality = 0\n",
    "        best_t = None\n",
    "        best_index = None\n",
    "        \n",
    "        feature_indexes = range(data.shape[1])\n",
    "        if self.max_features == 'sqrt':\n",
    "            n_indexes = int(np.sqrt(data.shape[1])) + 1\n",
    "            feature_indexes = np.random.choice(feature_indexes, n_indexes, replace=False)\n",
    "        \n",
    "        for index in feature_indexes:\n",
    "            # будем проверять только уникальные значения признака, исключая повторения\n",
    "            t_values = np.unique([row[index] for row in data])\n",
    "\n",
    "            for t in t_values:\n",
    "                true_data, false_data, true_labels, false_labels = self.split(data, labels, index, t)\n",
    "                #  пропускаем разбиения, в которых в узле остается менее 5 объектов\n",
    "                if len(true_data) < self.min_samples_leaf or len(false_data) < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                current_quality = self.quality(true_labels, false_labels, current_gini)\n",
    "\n",
    "                #  выбираем порог, на котором получается максимальный прирост качества\n",
    "                if current_quality > best_quality:\n",
    "                    best_quality, best_t, best_index = current_quality, t, index\n",
    "\n",
    "        return best_quality, best_t, best_index\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        quality, t, index = self.find_best_split(X, y)\n",
    "        #  Базовый случай - прекращаем рекурсию, когда нет прироста в качества или сработал критерий останова\n",
    "        if quality == 0 or self.max_depth and depth >= self.max_depth:\n",
    "            return Leaf(X, y)\n",
    "\n",
    "        true_data, false_data, true_labels, false_labels = self.split(X, y, index, t)\n",
    "\n",
    "        # Рекурсивно строим два поддерева\n",
    "        left_branch = self.build_tree(true_data, true_labels, depth + 1)\n",
    "        right_branch = self.build_tree(false_data, false_labels, depth + 1)\n",
    "\n",
    "        # Возвращаем класс узла со всеми поддеревьями, то есть целого дерева\n",
    "        return Node(index, t, left_branch, right_branch)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.tree.classify(sample) for sample in X])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return np.array([self.tree.probability(sample) for sample in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Класс случайного леса для решения задач классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_leaf=1, silent=True, random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.silent = silent\n",
    "        self.random_state = random_state\n",
    "        self.forest = []\n",
    "\n",
    "    def get_bootstrap(self, data, labels):\n",
    "        n_samples = data.shape[0]\n",
    "        bootstrap = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            b_data = np.zeros(data.shape)\n",
    "            b_labels = np.zeros(labels.shape)\n",
    "\n",
    "            if self.random_state:\n",
    "                np.random.seed(self.random_state)\n",
    "\n",
    "            for j in range(n_samples):\n",
    "                sample_index = np.random.randint(0, n_samples - 1)\n",
    "                b_data[j] = data[sample_index]\n",
    "                b_labels[j] = labels[sample_index]\n",
    "            bootstrap.append((b_data, b_labels))\n",
    "\n",
    "        return bootstrap\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        bootstrap = self.get_bootstrap(X, y)\n",
    "        for i, (b_data, b_labels) in enumerate(bootstrap):\n",
    "            tree = DecisionTree(self.max_depth, self.min_samples_leaf, max_features='sqrt')\n",
    "            tree.fit(b_data, b_labels)\n",
    "            self.forest.append(tree)\n",
    "            if not self.silent and i % (self.n_estimators / 10) == 0:\n",
    "                print(f'Построено деревьев {i}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = [tree.predict(X) for tree in self.forest]\n",
    "        preds_per_object = list(zip(*preds))\n",
    "        voted_predictions = np.array([max(set(obj), key=obj.count) for obj in preds_per_object])\n",
    "        return voted_predictions\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        preds = [tree.predict_proba(X) for tree in self.forest]\n",
    "        preds_per_object = list(zip(*preds))\n",
    "        predictions = np.array([sum(obj) / len(self.forest) for obj in preds_per_object])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Класс градиентного бустинга на решающих деревьях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_leaf=1, learning_rate=.3, random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        self.coefs = [1] * n_estimators\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i, x in enumerate(X):\n",
    "            for tree, coef in zip(self.trees, self.coefs):\n",
    "                preds[i] += self.learning_rate * coef * tree.predict_proba([x])[0]\n",
    "        return preds\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tree = DecisionTree(self.max_depth, self.min_samples_leaf)\n",
    "        tree.fit(X, y)\n",
    "        preds = self.learning_rate * self.coefs[0] * tree.predict_proba(X)\n",
    "        self.trees.append(tree)\n",
    "\n",
    "        for i in range(1, self.n_estimators):\n",
    "            tree = DecisionTree(self.max_depth, self.min_samples_leaf)\n",
    "            tree.fit(X, y - preds)\n",
    "            preds += self.learning_rate * self.coefs[i] * tree.predict_proba(X)\n",
    "            self.trees.append(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Масштабирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        self.std = 0\n",
    "        self.mean = 0\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.std = np.std(X, axis=0)\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Балансировка классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverSampling:\n",
    "    def __init__(self, sampling_strategy=1, algorithm='SMOTE', k_neighbors=5, random_state=None):\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.algorithm = algorithm\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.random_state = random_state\n",
    "\n",
    "    @staticmethod\n",
    "    def dist(x1, x2):\n",
    "        distance = 0\n",
    "        for i in range(len(x1)):\n",
    "            distance += np.square(x1[i] - x2[i])\n",
    "        return np.sqrt(distance)\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        need_samples = 0\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        data = np.c_[X, y]\n",
    "        classes = {0: np.sum(data[:, -1] == 0), 1: np.sum(data[:, -1] == 1)}\n",
    "        minor_class = min(classes, key=classes.get)\n",
    "\n",
    "        n_minor = classes[minor_class]\n",
    "        n_major = data.shape[0] - n_minor\n",
    "\n",
    "        if n_minor / n_major < self.sampling_strategy:\n",
    "            need_samples = int(n_major * self.sampling_strategy) - n_minor\n",
    "\n",
    "        if need_samples == 0:\n",
    "            return X, y\n",
    "\n",
    "        k = self.k_neighbors\n",
    "        if self.algorithm == 'SMOTE':\n",
    "            if n_minor - 1 < k:\n",
    "                k = n_minor - 1\n",
    "\n",
    "        minor_data = X[data[:, -1] == minor_class]\n",
    "        if need_samples < n_minor:\n",
    "            sample_indexes = np.random.choice(n_minor, need_samples, replace=False)\n",
    "        else:\n",
    "            sample_indexes = np.random.choice(n_minor, need_samples)\n",
    "\n",
    "        for i in sample_indexes:\n",
    "            current_sample = minor_data[sample_indexes[i]]\n",
    "            \n",
    "            if self.algorithm == 'SMOTE':\n",
    "                # Генерируем новый признак на основе kNN\n",
    "                distances = []\n",
    "    \n",
    "                for j, sample in enumerate(minor_data):\n",
    "                    if j == sample_indexes[i]:\n",
    "                        distances.append((np.inf, j))\n",
    "                    else:\n",
    "                        distance = self.dist(current_sample, sample)\n",
    "                        distances.append((distance, j))\n",
    "    \n",
    "                vectors = []\n",
    "                for j, d in enumerate(sorted(distances)[0:k]):\n",
    "                    vectors.append((minor_data[d[1]] - current_sample) / k * np.random.random() / (j + 1))\n",
    "    \n",
    "                current_sample += np.sum(vectors, axis=0)\n",
    "        \n",
    "            current_sample = np.append(current_sample, minor_class)\n",
    "            data = np.vstack((data, current_sample))\n",
    "        return data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление новых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(df_train, df_test, y, corr_limit=.1):\n",
    "    df_train_new = df_train.copy()\n",
    "    df_test_new = df_test.copy()\n",
    "\n",
    "    for col1 in range(df_train.shape[1]):\n",
    "        for col2 in range(df_train.shape[1]):\n",
    "\n",
    "            if col1 != col2:\n",
    "                test_feature = df_train[:, col1] / (df_train[:, col2] + 1e-8)\n",
    "                corr = np.corrcoef(y, test_feature)[0][1]\n",
    "                if np.abs(corr) > corr_limit:\n",
    "                    df_train_new = np.c_[df_train_new, test_feature]\n",
    "                    df_test_new = np.c_[df_test_new, df_test[:, col1] / (df_test[:, col2] + 1e-8)]\n",
    "\n",
    "            if col1 >= col2:\n",
    "                test_feature = df_train[:, col1] * df_train[:, col2]\n",
    "                corr = np.corrcoef(y, test_feature)[0][1]\n",
    "                if np.abs(corr) > corr_limit:\n",
    "                    df_train_new = np.c_[df_train_new, test_feature]\n",
    "                    df_test_new = np.c_[df_test_new, df_test[:, col1] * df_test[:, col2]]\n",
    "\n",
    "    return df_train_new, df_test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбираем лучшую модель перебором параметров на ооснове ROC AUC метрики для тестовой выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если `y_test` параметр не пуст, то функция возвращает значение метрики `ROC AUC` на тренировочной и тестовой выборках.\n",
    "\n",
    "Если `y_test` параметр не пуст, то функция возвращает результаты предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(model_class, params, X, y, X_test, y_test=None, sampling_algorithm=None, sampling_strategy=None,\n",
    "                new_features=False, random_state=None):\n",
    "    train_data, test_data, train_labels = X.copy(), X_test.copy(), y.copy()\n",
    "\n",
    "    if new_features:\n",
    "        train_data, test_data = add_new_features(train_data, test_data, y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "\n",
    "    if y_test is not None:\n",
    "        train_data_orig = train_data.copy()\n",
    "\n",
    "    if sampling_algorithm:\n",
    "        oversampler = OverSampling(random_state=random_state, algorithm=sampling_algorithm, sampling_strategy=sampling_strategy)\n",
    "        train_data, train_labels = oversampler.fit_resample(train_data, train_labels)\n",
    "\n",
    "    model = model_class(**params)\n",
    "    model.fit(train_data, train_labels)\n",
    "    preds = model.predict_proba(test_data)\n",
    "\n",
    "    if y_test is not None:\n",
    "        train_score = roc_auc_score(y, model.predict_proba(train_data_orig))\n",
    "        test_score = roc_auc_score(y_test, preds)\n",
    "        return train_score, test_score\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.001 (0.4002109475356384, 0.39888180715592375)\n",
      "300 0.001 (0.43044430952373514, 0.42730292955826904)\n",
      "1000 0.001 (0.5296566576562123, 0.5173653627672936)\n",
      "3000 0.001 (0.6738213005121956, 0.6524183129660837)\n",
      "10000 0.001 (0.8076493915493729, 0.8003986236553321)\n",
      "30000 0.001 (0.8338043528000744, 0.8207240112610005)\n",
      "100 0.003 (0.4304949728869325, 0.4273459548652894)\n",
      "300 0.003 (0.5183158272346674, 0.5072602298481657)\n",
      "1000 0.003 (0.6738576637325551, 0.6524438955810686)\n",
      "3000 0.003 (0.8022038971566539, 0.7942867043661456)\n",
      "10000 0.003 (0.8338029227857906, 0.8207251741071362)\n",
      "30000 0.003 (0.8495684216891792, 0.8333269376795903)\n",
      "100 0.01 (0.5300063982924796, 0.517674679839387)\n",
      "300 0.01 (0.6740002565854252, 0.6525485517332803)\n",
      "1000 0.01 (0.807675336094236, 0.8004242062703172)\n",
      "3000 0.01 (0.8338023099225261, 0.8207216855687292)\n",
      "10000 0.01 (0.8509563526954829, 0.8345258320454827)\n",
      "30000 0.01 (0.8609335623535412, 0.8447332954245694)\n",
      "100 0.03 (0.6743489757829174, 0.652904382650801)\n",
      "300 0.03 (0.8023005252646879, 0.7943960119029004)\n",
      "1000 0.03 (0.8337967941531458, 0.8207205227225933)\n",
      "3000 0.03 (0.849567400250405, 0.8333176349105047)\n",
      "10000 0.03 (0.8609341752168057, 0.8447332954245694)\n",
      "30000 0.03 (0.8624743006004646, 0.8470438706961733)\n",
      "100 0.1 (0.807895558293942, 0.8007311976501396)\n",
      "300 0.1 (0.8337798382694954, 0.8207030800305579)\n",
      "1000 0.1 (0.8509528798036509, 0.8345269948916184)\n",
      "3000 0.1 (0.860933970929051, 0.844734458270705)\n",
      "10000 0.1 (0.8625210824963195, 0.8470741046957001)\n",
      "30000 0.1 (0.8630595850180528, 0.8469834026971161)\n",
      "100 0.3 (0.8337199819573301, 0.820661217569673)\n",
      "300 0.3 (0.8495531001075669, 0.8332955408339268)\n",
      "1000 0.3 (0.8609341752168058, 0.8447332954245693)\n",
      "3000 0.3 (0.8624743006004646, 0.8470415450039019)\n",
      "10000 0.3 (0.8630595850180528, 0.8469834026971161)\n",
      "30000 0.3 (0.8635737772969575, 0.846748507777711)\n",
      "100 1 (0.7155895662889431, 0.7276091069458009)\n",
      "300 1 (0.7274215044731003, 0.7347408422959707)\n",
      "1000 1 (0.7399212593277845, 0.7429168134759939)\n",
      "3000 1 (0.7423328762735398, 0.7437482484630084)\n",
      "10000 1 (0.7438564543490486, 0.7438575559997624)\n",
      "30000 1 (0.7452090435737742, 0.7442657149933909)\n",
      "100 3 (0.705408681739299, 0.7129584084822675)\n",
      "300 3 (0.6836354885419158, 0.6900224313019625)\n",
      "1000 3 (0.6932743976779986, 0.696959971347472)\n",
      "3000 3 (0.6961366734108997, 0.6993612486176672)\n",
      "10000 3 (0.6963133823188267, 0.6988065710109407)\n",
      "30000 3 (0.6953064479752749, 0.6980228127154902)\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1e-3, 3e-3, .01, .03, .1, .3, 1, 3]:\n",
    "    for n_iter in [100, 300, 1000, 3000, 10000, 30000]:\n",
    "        y_pred = fit_predict(\n",
    "            LogisticRegression,\n",
    "            {'n_iter': n_iter, 'alpha': alpha},\n",
    "            train_data, train_labels, test_data, y_test=test_labels,\n",
    "            sampling_algorithm='Random',\n",
    "            sampling_strategy=1,\n",
    "            new_features=True,\n",
    "            random_state=1\n",
    "        )\n",
    "        print(n_iter, alpha, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 10 (0.8696478651112131, 0.8467008310861486)\n",
      "0.3 30 (0.8761060139046142, 0.8530488081408433)\n",
      "0.3 100 (0.8819625352600599, 0.8665110778537216)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for max_depth in [2, 3, 4]:\n",
    "    for learning_rate in [1e-2, 3e-2, .1, .3, 1]:\n",
    "        for n_estimators in [10, 30, 100, 300, 1000]:\n",
    "            preds = fit_predict(\n",
    "                GradientBoosting,\n",
    "                {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_leaf': 30, 'learning_rate': learning_rate},\n",
    "                train_data, train_labels, test_data, y_test=test_labels,\n",
    "                sampling_algorithm='Random',\n",
    "                sampling_strategy=1,\n",
    "                new_features=True\n",
    "            )\n",
    "            print(max_depth, learning_rate, n_estimators, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for max_depth in [6, 9, 12]:\n",
    "    for n_estimators in [50, 100, 200, 400, 1000]:\n",
    "        preds = fit_predict(\n",
    "            RandomForestClassifier,\n",
    "            {'n_estimators': n_estimators, 'max_depth': max_depth},\n",
    "            train_data, train_labels, test_data, y_test=test_labels,\n",
    "            sampling_algorithm='Random',\n",
    "            sampling_strategy=1,\n",
    "            new_features=True\n",
    "        )\n",
    "        print(sampling_strategy, n_estimators, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение выбранной модели и вывод результатов в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = fit_predict(\n",
    "#     GradientBoosting,\n",
    "#     {'n_estimators': 50, 'max_depth': 3, 'min_samples_leaf': 30, 'learning_rate': .3},\n",
    "#     X, y, df_test,\n",
    "#     sampling_algorithm='Random',\n",
    "#     sampling_strategy=.3,\n",
    "#     new_features=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fit_predict(\n",
    "    LogisticRegression,\n",
    "    {'n_iter': 10000, 'alpha': .3, 'random_state': 1},\n",
    "    X, y, X_test,\n",
    "    sampling_algorithm='Random',\n",
    "    sampling_strategy=1,\n",
    "    new_features=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробовал отправлять результаты, полученные с помощью лучших (среди проверенных) моделей Градиентного бустинга, Деревьев решений и Логистической регрессии, а также их комбинации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат получился на основе логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Id': df_test.index, TARGET: y_pred}).to_csv('Batorov_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
